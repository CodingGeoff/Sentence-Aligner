import streamlit as st
import pandas as pd
import numpy as np
import re
import os
from io import BytesIO
from sklearn.metrics.pairwise import cosine_similarity
from fastdtw import fastdtw
from sentence_transformers import SentenceTransformer
import torch
from datetime import datetime

# é…ç½®è®¾ç½®
os.environ["TRANSFORMERS_OFFLINE"] = "1"

def main():
    # ========== ç•Œé¢ç»„ä»¶ ==========
    st.title("ğŸŒ æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ")
    st.caption("ä¸“ä¸šçº§ä¸­è‹±æ–‡æ–‡æ¡£å¯¹é½å·¥å…· | æ”¯æŒGPUåŠ é€Ÿ")

    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        use_gpu = st.checkbox("å¯ç”¨GPUåŠ é€Ÿ", value=torch.cuda.is_available())
        
        st.markdown("---")
        st.header("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
        zh_file = st.file_uploader("ä¸Šä¼ ä¸­æ–‡æ–‡æ¡£", type=["txt"], key="zh")
        en_file = st.file_uploader("ä¸Šä¼ è‹±æ–‡æ–‡æ¡£", type=["txt"], key="en")

    # ========== æ ¸å¿ƒåŠŸèƒ½ ==========
    # def split_sentences(text, lang):
    #     """æ™ºèƒ½åˆ†å¥å‡½æ•°"""
    #     text = re.sub(r'([ã€‚ï¼ï¼Ÿ?ï¼])([^â€â€™])', r'\1\n\2', text) if lang == "zh" \
    #         else re.sub(r'([.!?])([â€™"])', r'\1\n\2', text)
    #     return [s.strip() for s in re.split(r'\n+', text) if s.strip() else []
    
    def split_sentences(text, lang):
    # """æ™ºèƒ½åˆ†å¥å‡½æ•°"""
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ?ï¼])([^â€â€™])', r'\1\n\2', text) if lang == "zh" \
            else re.sub(r'([.!?])([â€™"])', r'\1\n\2', text)
        return [s.strip() for s in re.split(r'\n+', text) if s.strip()]

    def load_model():
        """æ¨¡å‹åŠ è½½å‡½æ•°"""
        model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ {model_path} ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å·²æ­£ç¡®æ”¾ç½®")
            
        device = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"
        return SentenceTransformer(model_path, device=device)

    def align_texts(zh_sentences, en_sentences):
        """GPUåŠ é€Ÿçš„å¯¹é½å‡½æ•°"""
        # åŠ è½½æ¨¡å‹
        model = load_model()
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        zh_embeddings = model.encode(zh_sentences)
        en_embeddings = model.encode(en_sentences)

        # å¯¹é½è®¡ç®—
        def cosine_distance(a, b):
            return 1 - cosine_similarity([a], [b])[0][0]

        _, path = fastdtw(zh_embeddings, en_embeddings, 
                         dist=cosine_distance, radius=3)

        # æ„å»ºå¯¹é½ç»“æœ
        aligned_pairs = []
        last_zh, last_en = -1, -1
        
        for i, j in path:
            if i != last_zh and j != last_en:
                aligned_pairs.append((zh_sentences[i], en_sentences[j]))
                last_zh, last_en = i, j
            elif i == last_zh:
                aligned_pairs[-1] = (aligned_pairs[-1][0], 
                                    aligned_pairs[-1][1] + " " + en_sentences[j])
                last_en = j
            elif j == last_en:
                aligned_pairs[-1] = (aligned_pairs[-1][0] + zh_sentences[i], 
                                    aligned_pairs[-1][1])
                last_zh = i
        return aligned_pairs

    # ========== æ–‡ä»¶å¤„ç† ==========
    @st.cache_data(show_spinner=False)
    def load_file(uploaded_file, default_path):
        """å¸¦ç¼“å­˜çš„æ–‡ä»¶åŠ è½½"""
        if uploaded_file is not None:
            return uploaded_file.read().decode("utf-8")
        try:
            with open(default_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            return None

    # ========== ä¸»æµç¨‹ ==========
    zh_text = load_file(zh_file, "zn.txt")
    en_text = load_file(en_file, "en.txt")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'processed' not in st.session_state:
        st.session_state.processed = False
    if 'df' not in st.session_state:
        st.session_state.df = pd.DataFrame()
    if 'aligned' not in st.session_state:
        st.session_state.aligned = []

    if not zh_text or not en_text:
        missing = []
        if not zh_text: missing.append("ä¸­æ–‡æ–‡æ¡£")
        if not en_text: missing.append("è‹±æ–‡æ–‡æ¡£")
        st.error(f"âŒ ç¼ºå°‘ {' å’Œ '.join(missing)}")
        return

    if st.button("ğŸš€ å¼€å§‹å¤„ç†", use_container_width=True, type="primary"):
        with st.status("ğŸ“Š å¤„ç†æµç¨‹", expanded=True) as status:
            try:
                # åˆ†å¥å¤„ç†
                st.write("## é˜¶æ®µ 1/4ï¼šåˆ†å¥å¤„ç†")
                zh_sents = split_sentences(zh_text, "zh")
                en_sents = split_sentences(en_text, "en")
                st.write(f"- ä¸­æ–‡å¥å­æ•°ï¼š{len(zh_sents)}")
                st.write(f"- è‹±æ–‡å¥å­æ•°ï¼š{len(en_sents)}")

                # æ‰§è¡Œå¯¹é½
                st.write("## é˜¶æ®µ 2/4ï¼šè¯­ä¹‰å¯¹é½")
                aligned = align_texts(zh_sents, en_sents)
                
                # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€
                st.session_state.df = pd.DataFrame(aligned, columns=["ä¸­æ–‡", "English"])
                st.session_state.aligned = aligned
                st.session_state.processed = True

                total_time = (datetime.now() - process_start).total_seconds()
                status.update(
                    label=f"âœ… å¤„ç†å®Œæˆ | æ€»è€—æ—¶ {total_time:.2f}s",
                    state="complete",
                    expanded=False
                )

            except Exception as e:
                status.update(label="âŒ å¤„ç†å¤±è´¥", state="error")
                st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
                st.stop()

    # æ˜¾ç¤ºç»“æœå’Œå¯¼å‡ºé€‰é¡¹ï¼ˆç‹¬ç«‹äºå¤„ç†æŒ‰é’®ï¼‰
    if st.session_state.processed:
        st.success(f"æˆåŠŸå¯¹é½ {len(st.session_state.aligned)} å¯¹å¥å­")
        
        with st.container():
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.subheader("ğŸ“„ æ•°æ®é¢„è§ˆ")
                st.dataframe(
                    st.session_state.df,
                    height=600,
                    use_container_width=True,
                    hide_index=True
                )
            
            with col2:
                st.subheader("ğŸ“¥ å¯¼å‡ºé€‰é¡¹")
                export_format = st.radio("é€‰æ‹©æ ¼å¼", 
                    ["Excel", "CSV", "TXT", "HTML", "Markdown"],
                    index=0,
                    label_visibility="collapsed")
                
                buffer = BytesIO()
                if export_format == "Excel":
                    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                        st.session_state.df.to_excel(writer, index=False)
                    mime_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    ext = "xlsx"
                elif export_format == "CSV":
                    buffer.write(st.session_state.df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8'))
                    mime_type = "text/csv"
                    ext = "csv"
                elif export_format == "TXT":
                    txt_content = "\n".join([f"{zh}\t{en}" for zh, en in st.session_state.aligned])
                    buffer.write(txt_content.encode("utf-8"))
                    mime_type = "text/plain"
                    ext = "txt"
                elif export_format == "HTML":
                    html_content = st.session_state.df.to_html(index=False, border=0)
                    buffer.write(html_content.encode("utf-8"))
                    mime_type = "text/html"
                    ext = "html"
                elif export_format == "Markdown":
                    md_content = st.session_state.df.to_markdown(index=False)
                    buffer.write(md_content.encode("utf-8"))
                    mime_type = "text/markdown"
                    ext = "md"

                st.download_button(
                    label=f"ä¸‹è½½ {export_format} æ–‡ä»¶",
                    data=buffer.getvalue(),
                    file_name=f"aligned_text.{ext}",
                    mime=mime_type,
                    use_container_width=True
                )

if __name__ == '__main__':
    main()