# # # # import streamlit as st
# # # # import pandas as pd
# # # # import numpy as np
# # # # from sklearn.metrics.pairwise import cosine_similarity
# # # # from fastdtw import fastdtw
# # # # from sentence_transformers import SentenceTransformer
# # # # import re

# # # # def main():
# # # #     # ä¸­è‹±æ–‡åˆ†å¥å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
# # # #     def split_sentences(text, lang):
# # # #         # å¤„ç†ç‰¹æ®Šç¬¦å·åçš„æ¢è¡Œ
# # # #         text = re.sub(r'([ã€‚ï¼ï¼Ÿ?\!])([^â€â€™])', r'\1\n\2', text)
# # # #         # å¤„ç†è‹±æ–‡å¼•å·
# # # #         text = re.sub(r'([.!?])([â€™"])', r'\1\n\2', text)
# # # #         # æ‹†åˆ†å¥å­
# # # #         sentences = [s.strip() for s in re.split(r'\n+', text) if s.strip()]
# # # #         return sentences


# # # #     # å¯¹é½ç®—æ³•æ ¸å¿ƒ
# # # #     def align_texts(zh_sentences, en_sentences):
# # # #         # åŠ è½½å¤šè¯­è¨€å¥å‘é‡æ¨¡å‹
# # # #         model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# # # #         # ç”ŸæˆåµŒå…¥å‘é‡
# # # #         zh_embeddings = model.encode(zh_sentences)
# # # #         en_embeddings = model.encode(en_sentences)

# # # #         # æ„å»ºæˆæœ¬çŸ©é˜µ
# # # #         cost_matrix = np.zeros((len(zh_sentences), len(en_sentences)))
# # # #         for i, zh_vec in enumerate(zh_embeddings):
# # # #             for j, en_vec in enumerate(en_embeddings):
# # # #                 cost_matrix[i, j] = 1 - cosine_similarity([zh_vec], [en_vec])[0][0]

# # # #         # ä½¿ç”¨DTWå¯»æ‰¾æœ€ä¼˜è·¯å¾„
# # # #         distance, path = fastdtw(cost_matrix, radius=3)

# # # #         # å¤„ç†å¯¹é½è·¯å¾„
# # # #         aligned_pairs = []
# # # #         last_zh = last_en = -1

# # # #         for (i, j) in path:
# # # #             if i != last_zh and j != last_en:
# # # #                 # æ–°çš„åŒ¹é…å¯¹
# # # #                 aligned_pairs.append((zh_sentences[i], en_sentences[j]))
# # # #                 last_zh = i
# # # #                 last_en = j
# # # #             elif i == last_zh:
# # # #                 # è‹±æ–‡åˆå¹¶
# # # #                 aligned_pairs[-1] = (aligned_pairs[-1][0], aligned_pairs[-1][1] + " " + en_sentences[j])
# # # #                 last_en = j
# # # #             elif j == last_en:
# # # #                 # ä¸­æ–‡åˆå¹¶
# # # #                 aligned_pairs[-1] = (aligned_pairs[-1][0] + en_sentences[j], aligned_pairs[-1][1])
# # # #                 last_zh = i

# # # #         return aligned_pairs


# # # #     # Streamlitç•Œé¢
# # # #     st.title("ä¸­è‹±æ–‡æ–‡æ¡£å¯¹é½å·¥å…·")

# # # #     # æ–‡ä»¶ä¸Šä¼ 
# # # #     zh_file = st.file_uploader("ä¸Šä¼ ä¸­æ–‡æ–‡æ¡£", type=["txt"])
# # # #     en_file = st.file_uploader("ä¸Šä¼ è‹±æ–‡æ–‡æ¡£", type=["txt"])

# # # #     # ä½¿ç”¨é»˜è®¤æ–‡ä»¶æˆ–ä¸Šä¼ æ–‡ä»¶
# # # #     zh_text = ""
# # # #     en_text = ""

# # # #     if zh_file is None:
# # # #         with open("zn.txt", "r", encoding="utf-8") as f:
# # # #             zh_text = f.read()
# # # #     else:
# # # #         zh_text = zh_file.read().decode("utf-8")

# # # #     if en_file is None:
# # # #         with open("en.txt", "r", encoding="utf-8") as f:
# # # #             en_text = f.read()
# # # #     else:
# # # #         en_text = en_file.read().decode("utf-8")

# # # #     # æ‰§è¡Œå¯¹é½
# # # #     if st.button("å¼€å§‹å¯¹é½"):
# # # #         with st.spinner("å¤„ç†ä¸­..."):
# # # #             # åˆ†å¥å¤„ç†
# # # #             zh_sentences = split_sentences(zh_text, "zh")
# # # #             en_sentences = split_sentences(en_text, "en")

# # # #             # æ‰§è¡Œå¯¹é½ç®—æ³•
# # # #             aligned = align_texts(zh_sentences, en_sentences)

# # # #             # åˆ›å»ºDataFrame
# # # #             df = pd.DataFrame(aligned, columns=["ä¸­æ–‡", "English"])

# # # #             # æ˜¾ç¤ºç»“æœ
# # # #             st.dataframe(df)

# # # #             # ä¸‹è½½æŒ‰é’®
# # # #             st.download_button(
# # # #                 label="ä¸‹è½½Excelæ–‡ä»¶",
# # # #                 data=df.to_excel(index=False),
# # # #                 file_name="aligned_text.xlsx",
# # # #                 mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
# # # #             )


# # # # if __name__ == '__main__':
# # # #     from streamlit.web import cli as stcli
# # # #     from streamlit import runtime
# # # #     import sys

# # # #     # è®¾ç½®é¡µé¢é…ç½®
# # # #     st.set_page_config(
# # # #         page_title="æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ",
# # # #         page_icon="ğŸŒ",
# # # #         layout="wide",
# # # #         initial_sidebar_state="expanded"
# # # #     )

# # # #     if runtime.exists():
# # # #         main()
# # # #     else:
# # # #         sys.argv = ["streamlit", "run", sys.argv[0]]
# # # #         sys.exit(stcli.main())


# # # import streamlit as st
# # # import pandas as pd
# # # import numpy as np
# # # from sklearn.metrics.pairwise import cosine_similarity
# # # from fastdtw import fastdtw
# # # from sentence_transformers import SentenceTransformer
# # # import re
# # # import os  # æ–°å¢å¯¼å…¥

# # # # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰
# # # os.environ["TRANSFORMERS_OFFLINE"] = "1"
# # # os.environ["HF_DATASETS_OFFLINE"] = "1"

# # # def main():
# # #     # ä¸­è‹±æ–‡åˆ†å¥å‡½æ•°ä¿æŒä¸å˜
# # #     def split_sentences(text, lang):
# # #         text = re.sub(r'([ã€‚ï¼ï¼Ÿ?\!])([^â€â€™])', r'\1\n\2', text)
# # #         text = re.sub(r'([.!?])([â€™"])', r'\1\n\2', text)
# # #         return [s.strip() for s in re.split(r'\n+', text) if s.strip()]

# # #     # ä¿®æ”¹åçš„å¯¹é½å‡½æ•°
# # #     def align_texts(zh_sentences, en_sentences):
# # #         # ä¿®æ”¹æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆå…³é”®ä¿®æ”¹ï¼‰
# # #         model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
        
# # #         # éªŒè¯æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆæ–°å¢æ ¡éªŒï¼‰
# # #         if not os.path.exists(model_path):
# # #             st.error(f"æ¨¡å‹è·¯å¾„ {model_path} ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
# # #             return []
            
# # #         try:
# # #             model = SentenceTransformer(model_path)
# # #         except Exception as e:
# # #             st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
# # #             return []

# # #         # ä»¥ä¸‹ä»£ç ä¿æŒä¸å˜...
# # #         zh_embeddings = model.encode(zh_sentences)
# # #         en_embeddings = model.encode(en_sentences)
        
# # #         distance, path = fastdtw(
# # #             zh_embeddings, 
# # #             en_embeddings,
# # #             dist=cosine_distance,  # ä½¿ç”¨è‡ªå®šä¹‰è·ç¦»å‡½æ•°
# # #             radius=3
# # #         )

# # #         # cost_matrix = np.zeros((len(zh_sentences), len(en_sentences)))
# # #         # for i, zh_vec in enumerate(zh_embeddings):
# # #         #     for j, en_vec in enumerate(en_embeddings):
# # #         #         cost_matrix[i, j] = 1 - cosine_similarity([zh_vec], [en_vec])[0][0]
# # #         def cosine_distance(vec1, vec2):
# # #             return 1 - cosine_similarity([vec1], [vec2])[0][0]

# # #         # # distance, path = fastdtw(cost_matrix, radius=3)
# # #         # distance, path = fastdtw(zh_embeddings, en_embeddings, dist=cosine_distance, radius=3)

# # #         aligned_pairs = []
# # #         last_zh = last_en = -1

# # #         for (i, j) in path:
# # #             if i != last_zh and j != last_en:
# # #                 aligned_pairs.append((zh_sentences[i], en_sentences[j]))
# # #                 last_zh = i
# # #                 last_en = j
# # #             elif i == last_zh:
# # #                 aligned_pairs[-1] = (aligned_pairs[-1][0], aligned_pairs[-1][1] + " " + en_sentences[j])
# # #                 last_en = j
# # #             elif j == last_en:
# # #                 aligned_pairs[-1] = (aligned_pairs[-1][0] + en_sentences[j], aligned_pairs[-1][1])
# # #                 last_zh = i

# # #         return aligned_pairs

# # #     # ç•Œé¢éƒ¨åˆ†ä¿æŒä¸å˜...
# # #     st.title("ä¸­è‹±æ–‡æ–‡æ¡£å¯¹é½å·¥å…·")

# # #     zh_file = st.file_uploader("ä¸Šä¼ ä¸­æ–‡æ–‡æ¡£", type=["txt"])
# # #     en_file = st.file_uploader("ä¸Šä¼ è‹±æ–‡æ–‡æ¡£", type=["txt"])

# # #     zh_text = ""
# # #     en_text = ""

# # #     if zh_file is None:
# # #         try:
# # #             with open("zn.txt", "r", encoding="utf-8") as f:
# # #                 zh_text = f.read()
# # #         except FileNotFoundError:
# # #             st.warning("é»˜è®¤ä¸­æ–‡æ–‡ä»¶ zn.txt ä¸å­˜åœ¨")
# # #     else:
# # #         zh_text = zh_file.read().decode("utf-8")

# # #     if en_file is None:
# # #         try:
# # #             with open("en.txt", "r", encoding="utf-8") as f:
# # #                 en_text = f.read()
# # #         except FileNotFoundError:
# # #             st.warning("é»˜è®¤è‹±æ–‡æ–‡ä»¶ en.txt ä¸å­˜åœ¨")
# # #     else:
# # #         en_text = en_file.read().decode("utf-8")

# # #     if st.button("å¼€å§‹å¯¹é½"):
# # #         if not zh_text or not en_text:
# # #             st.error("è¯·å…ˆä¸Šä¼ æˆ–æä¾›ä¸­è‹±æ–‡æ–‡æ¡£")
# # #             return
            
# # #         with st.spinner("å¤„ç†ä¸­..."):
# # #             zh_sentences = split_sentences(zh_text, "zh")
# # #             en_sentences = split_sentences(en_text, "en")

# # #             if not zh_sentences or not en_sentences:
# # #                 st.error("åˆ†å¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹")
# # #                 return

# # #             aligned = align_texts(zh_sentences, en_sentences)

# # #             if not aligned:
# # #                 return

# # #             df = pd.DataFrame(aligned, columns=["ä¸­æ–‡", "English"])
# # #             st.dataframe(df)

# # #             st.download_button(
# # #                 label="ä¸‹è½½Excelæ–‡ä»¶",
# # #                 data=df.to_excel(index=False),
# # #                 file_name="aligned_text.xlsx",
# # #                 mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
# # #             )

# # # # ä»¥ä¸‹éƒ¨åˆ†ä¿æŒä¸å˜...
# # # if __name__ == '__main__':
# # #     from streamlit.web import cli as stcli
# # #     from streamlit import runtime
# # #     import sys

# # #     st.set_page_config(
# # #         page_title="æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ",
# # #         page_icon="ğŸŒ",
# # #         layout="wide",
# # #         initial_sidebar_state="expanded"
# # #     )

# # #     if runtime.exists():
# # #         main()
# # #     else:
# # #         sys.argv = ["streamlit", "run", sys.argv[0]]
# # #         sys.exit(stcli.main())



# # import streamlit as st
# # import pandas as pd
# # import numpy as np
# # from sklearn.metrics.pairwise import cosine_similarity
# # from fastdtw import fastdtw
# # from sentence_transformers import SentenceTransformer
# # import re
# # import os

# # # è®¾ç½®ç¦»çº¿æ¨¡å¼
# # os.environ["TRANSFORMERS_OFFLINE"] = "1"
# # os.environ["HF_DATASETS_OFFLINE"] = "1"

# # def main():
# #     # st.set_page_config(
# #     #     page_title="æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ",
# #     #     page_icon="ğŸŒ",
# #     #     layout="wide",
# #     #     initial_sidebar_state="expanded"
# #     # )

# #     # ========== ç•Œé¢ç¾åŒ– ==========
# #     st.title("ğŸŒ æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ")
# #     st.caption("ä¸Šä¼ ä¸­è‹±æ–‡æ–‡æ¡£ï¼Œè‡ªåŠ¨å®ç°å¥å­çº§å¯¹é½")

# #     # ä½¿ç”¨ä¾§è¾¹æ è¿›è¡Œæ–‡ä»¶ä¸Šä¼ 
# #     with st.sidebar:
# #         st.header("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
# #         zh_file = st.file_uploader("é€‰æ‹©ä¸­æ–‡æ–‡æ¡£", type=["txt"], key="zh")
# #         en_file = st.file_uploader("é€‰æ‹©è‹±æ–‡æ–‡æ¡£", type=["txt"], key="en")
        
# #         st.markdown("---")
# #         st.markdown("**é»˜è®¤æ–‡ä»¶**")
# #         st.caption("å½“æœªä¸Šä¼ æ–‡ä»¶æ—¶ï¼Œè‡ªåŠ¨ä½¿ç”¨ä»¥ä¸‹æ–‡ä»¶ï¼š")
# #         col1, col2 = st.columns(2)
# #         with col1:
# #             st.code("zn.txt\n(ä¸­æ–‡ç¤ºä¾‹)")
# #         with col2:
# #             st.code("en.txt\n(è‹±æ–‡ç¤ºä¾‹)")

# #     # ========== æ ¸å¿ƒåŠŸèƒ½ ==========
# #     def split_sentences(text, lang):
# #         """æ”¹è¿›ç‰ˆåˆ†å¥å‡½æ•°"""
# #         text = re.sub(r'([ã€‚ï¼ï¼Ÿ?\!])([^â€â€™])', r'\1\n\2', text)
# #         text = re.sub(r'([.!?])([â€™"])', r'\1\n\2', text)
# #         return [s.strip() for s in re.split(r'\n+', text) if s.strip()]

# #     def align_texts(zh_sentences, en_sentences):
# #         """ä¼˜åŒ–åçš„å¯¹é½å‡½æ•°"""
# #         # å®šä¹‰ä½™å¼¦è·ç¦»å‡½æ•°
# #         def cosine_distance(vec1, vec2):
# #             return 1 - cosine_similarity([vec1], [vec2])[0][0]

# #         # åŠ è½½æœ¬åœ°æ¨¡å‹
# #         model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
# #         if not os.path.exists(model_path):
# #             st.error(f"âŒ æ¨¡å‹è·¯å¾„ {model_path} ä¸å­˜åœ¨ï¼")
# #             return []

# #         try:
# #             with st.spinner("âš™ï¸ æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡å‹..."):
# #                 model = SentenceTransformer(model_path)
# #         except Exception as e:
# #             st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
# #             return []

# #         # ç”ŸæˆåµŒå…¥å‘é‡
# #         with st.spinner("ğŸ”§ æ­£åœ¨åˆ†ææ–‡æœ¬ç‰¹å¾..."):
# #             zh_embeddings = model.encode(zh_sentences)
# #             en_embeddings = model.encode(en_sentences)

# #         # æ‰§è¡ŒåŠ¨æ€æ—¶é—´è§„æ•´
# #         with st.spinner("â³ æ­£åœ¨è®¡ç®—æœ€ä½³å¯¹é½è·¯å¾„..."):
# #             try:
# #                 distance, path = fastdtw(
# #                     zh_embeddings,
# #                     en_embeddings,
# #                     dist=cosine_distance,
# #                     radius=3
# #                 )
# #             except Exception as e:
# #                 st.error(f"âŒ å¯¹é½è®¡ç®—å¤±è´¥ï¼š{str(e)}")
# #                 return []

# #         # å¤„ç†å¯¹é½ç»“æœ
# #         aligned_pairs = []
# #         last_zh = last_en = -1

# #         for (i, j) in path:
# #             if i != last_zh and j != last_en:
# #                 aligned_pairs.append((zh_sentences[i], en_sentences[j]))
# #                 last_zh = i
# #                 last_en = j
# #             elif i == last_zh:
# #                 aligned_pairs[-1] = (aligned_pairs[-1][0], aligned_pairs[-1][1] + " " + en_sentences[j])
# #                 last_en = j
# #             elif j == last_en:
# #                 aligned_pairs[-1] = (aligned_pairs[-1][0] + zh_sentences[i], aligned_pairs[-1][1])
# #                 last_zh = i

# #         return aligned_pairs

# #     # ========== æ–‡ä»¶å¤„ç† ==========
# #     @st.cache_data
# #     def load_default_file(filename):
# #         """å¸¦ç¼“å­˜çš„é»˜è®¤æ–‡ä»¶åŠ è½½"""
# #         try:
# #             with open(filename, "r", encoding="utf-8") as f:
# #                 return f.read()
# #         except FileNotFoundError:
# #             return None

# #     # è·å–æ–‡æœ¬å†…å®¹
# #     zh_text = zh_file.read().decode("utf-8") if zh_file else load_default_file("zn.txt")
# #     en_text = en_file.read().decode("utf-8") if en_file else load_default_file("en.txt")

# #     # éªŒè¯æ–‡æœ¬å†…å®¹
# #     if not zh_text or not en_text:
# #         missing_files = []
# #         if not zh_text: missing_files.append("ä¸­æ–‡æ–‡æ¡£")
# #         if not en_text: missing_files.append("è‹±æ–‡æ–‡æ¡£")
# #         st.error(f"âŒ ç¼ºå°‘{'å’Œ'.join(missing_files)}ï¼Œè¯·ä¸Šä¼ æ–‡ä»¶æˆ–ç¡®ä¿é»˜è®¤æ–‡ä»¶å­˜åœ¨")
# #         return

# #     # ========== æ‰§è¡Œå¯¹é½ ==========
# #     if st.button("ğŸš€ å¼€å§‹å¯¹é½", use_container_width=True):
# #         with st.status("ğŸ“Š æ­£åœ¨å¤„ç†æ–‡æ¡£...", expanded=True) as status:
# #             # åˆ†å¥å¤„ç†
# #             st.write("ğŸ”  æ­£åœ¨æ‹†åˆ†å¥å­...")
# #             zh_sentences = split_sentences(zh_text, "zh")
# #             en_sentences = split_sentences(en_text, "en")

# #             if not zh_sentences or not en_sentences:
# #                 st.error("âŒ åˆ†å¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹")
# #                 return

# #             # æ‰§è¡Œå¯¹é½
# #             st.write("ğŸ” æ­£åœ¨å¯¹é½å¥å­...")
# #             aligned = align_texts(zh_sentences, en_sentences)

# #             if not aligned:
# #                 status.update(label="å¤„ç†å¤±è´¥", state="error")
# #                 return

# #             # æ˜¾ç¤ºç»“æœ
# #             status.update(label="å¤„ç†å®Œæˆ!", state="complete")
            
# #             # ç»“æœå±•ç¤º
# #             st.success(f"âœ… æˆåŠŸå¯¹é½ {len(aligned)} å¯¹å¥å­")
# #             df = pd.DataFrame(aligned, columns=["ä¸­æ–‡", "English"])
            
# #             # åˆ†é¡µæ˜¾ç¤ºè¡¨æ ¼
# #             tab1, tab2 = st.tabs(["ğŸ“„ è¡¨æ ¼é¢„è§ˆ", "ğŸ“¥ æ•°æ®ä¸‹è½½"])
# #             with tab1:
# #                 st.dataframe(
# #                     df,
# #                     use_container_width=True,
# #                     height=600,
# #                     hide_index=True,
# #                     column_config={
# #                         "ä¸­æ–‡": st.column_config.TextColumn(width="large"),
# #                         "English": st.column_config.TextColumn(width="large")
# #                     }
# #                 )
# #             with tab2:
# #                 st.download_button(
# #                     label="ğŸ’¾ ä¸‹è½½Excelæ–‡ä»¶",
# #                     data=df.to_excel(index=False),
# #                     file_name="aligned_text.xlsx",
# #                     mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
# #                     use_container_width=True
# #                 )

# #             # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
# #             with st.expander("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯", expanded=True):
# #                 col1, col2, col3 = st.columns(3)
# #                 col1.metric("ä¸­æ–‡å¥å­æ•°", len(zh_sentences))
# #                 col2.metric("è‹±æ–‡å¥å­æ•°", len(en_sentences))
# #                 col3.metric("å¯¹é½ç‡", f"{len(aligned)/max(len(zh_sentences), len(en_sentences))*100:.1f}%")
# import streamlit as st
# import pandas as pd
# import numpy as np
# import re
# import os
# from io import BytesIO
# from sklearn.metrics.pairwise import cosine_similarity
# from fastdtw import fastdtw
# from sentence_transformers import SentenceTransformer
# import torch

# # é…ç½®è®¾ç½®
# os.environ["TRANSFORMERS_OFFLINE"] = "1"
# # st.set_page_config(
# #     page_title="æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ",
# #     page_icon="ğŸŒ",
# #     layout="wide",
# #     initial_sidebar_state="expanded"
# # )

# def main():
#     # ========== ç•Œé¢ç»„ä»¶ ==========
#     st.title("ğŸŒ æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ")
#     st.caption("ä¸“ä¸šçº§ä¸­è‹±æ–‡æ–‡æ¡£å¯¹é½å·¥å…· | æ”¯æŒGPUåŠ é€Ÿ")

#     with st.sidebar:
#         st.header("âš™ï¸ è®¾ç½®")
#         use_gpu = st.checkbox("å¯ç”¨GPUåŠ é€Ÿ", value=torch.cuda.is_available())
#         export_format = st.selectbox("å¯¼å‡ºæ ¼å¼", ["Excel", "CSV", "TXT", "HTML", "Markdown"], index=0)
        
#         st.markdown("---")
#         st.header("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
#         zh_file = st.file_uploader("ä¸Šä¼ ä¸­æ–‡æ–‡æ¡£", type=["txt"], key="zh")
#         en_file = st.file_uploader("ä¸Šä¼ è‹±æ–‡æ–‡æ¡£", type=["txt"], key="en")

#     # ========== æ ¸å¿ƒåŠŸèƒ½ ==========
#     def split_sentences(text, lang):
#         """æ™ºèƒ½åˆ†å¥å‡½æ•°"""
#         text = re.sub(r'([ã€‚ï¼ï¼Ÿ?ï¼])([^â€â€™])', r'\1\n\2', text) if lang == "zh" \
#             else re.sub(r'([.!?])([â€™"])', r'\1\n\2', text)
#         return [s.strip() for s in re.split(r'\n+', text) if s.strip()]

#     def align_texts(zh_sentences, en_sentences):
#         """GPUåŠ é€Ÿçš„å¯¹é½å‡½æ•°"""
#         # è®¾å¤‡é€‰æ‹©
#         device = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"
        
#         # åŠ è½½æ¨¡å‹
#         model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
#         if not os.path.exists(model_path):
#             raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ {model_path} ä¸å­˜åœ¨")

#         try:
#             model = SentenceTransformer(model_path, device=device)
#         except Exception as e:
#             raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

#         # ç”ŸæˆåµŒå…¥å‘é‡
#         with st.spinner(f"æ­£åœ¨ç¼–ç æ–‡æœ¬ ({device.upper()})..."):
#             zh_embeddings = model.encode(zh_sentences, show_progress_bar=False)
#             en_embeddings = model.encode(en_sentences, show_progress_bar=False)

#         # å¯¹é½è®¡ç®—
#         def cosine_distance(a, b):
#             return 1 - cosine_similarity([a], [b])[0][0]

#         with st.spinner("è®¡ç®—åŠ¨æ€æ—¶é—´è§„æ•´è·¯å¾„..."):
#             _, path = fastdtw(zh_embeddings, en_embeddings, dist=cosine_distance, radius=3)

#         # æ„å»ºå¯¹é½ç»“æœ
#         aligned = []
#         last_zh, last_en = -1, -1
#         for i, j in path:
#             if i != last_zh and j != last_en:
#                 aligned.append((zh_sentences[i], en_sentences[j]))
#                 last_zh, last_en = i, j
#             elif i == last_zh:
#                 aligned[-1] = (aligned[-1][0], aligned[-1][1] + " " + en_sentences[j])
#                 last_en = j
#             elif j == last_en:
#                 aligned[-1] = (aligned[-1][0] + zh_sentences[i], aligned[-1][1])
#                 last_zh = i
#         return aligned

#     # ========== æ–‡ä»¶å¤„ç† ==========
#     @st.cache_data
#     def load_file(uploaded_file, default_path):
#         """å¸¦ç¼“å­˜çš„æ–‡ä»¶åŠ è½½"""
#         if uploaded_file:
#             return uploaded_file.read().decode("utf-8")
#         try:
#             with open(default_path, "r", encoding="utf-8") as f:
#                 return f.read()
#         except FileNotFoundError:
#             return None

#     # åŠ è½½æ–‡æœ¬
#     zh_text = load_file(zh_file, "zn.txt")
#     en_text = load_file(en_file, "en.txt")

#     # éªŒè¯è¾“å…¥
#     if not zh_text or not en_text:
#         missing = []
#         if not zh_text: missing.append("ä¸­æ–‡æ–‡æ¡£")
#         if not en_text: missing.append("è‹±æ–‡æ–‡æ¡£")
#         st.error(f"ç¼ºå°‘ {' å’Œ '.join(missing)}ï¼Œè¯·ä¸Šä¼ æ–‡ä»¶æˆ–æ£€æŸ¥é»˜è®¤æ–‡ä»¶")
#         return

#     # ========== æ‰§è¡Œå¯¹é½ ==========
#     if st.button("ğŸš€ å¼€å§‹å¤„ç†", use_container_width=True):
#         with st.status("ğŸ“Š å¤„ç†æµç¨‹", expanded=True) as status:
#             # åˆ†å¥å¤„ç†
#             st.write("ğŸ”  åˆ†å¥å¤„ç†ä¸­...")
#             zh_sents = split_sentences(zh_text, "zh")
#             en_sents = split_sentences(en_text, "en")
#             st.write(f"è¯†åˆ«åˆ° {len(zh_sents)} æ¡ä¸­æ–‡å¥å­ | {len(en_sents)} æ¡è‹±æ–‡å¥å­")

#             # æ‰§è¡Œå¯¹é½
#             try:
#                 st.write("ğŸ” å¯¹é½å¤„ç†ä¸­...")
#                 aligned = align_texts(zh_sents, en_sents)
#             except Exception as e:
#                 status.update(label="å¤„ç†å¤±è´¥ âŒ", state="error")
#                 st.error(str(e))
#                 return

#             # ç»“æœå¤„ç†
#             status.update(label="å¤„ç†å®Œæˆ âœ…", state="complete")
#             df = pd.DataFrame(aligned, columns=["ä¸­æ–‡", "English"])
            
#             # ç»“æœæ˜¾ç¤º
#             tab1, tab2 = st.tabs(["ğŸ“„ æ•°æ®é¢„è§ˆ", "ğŸ“¥ å¯¼å‡ºé€‰é¡¹"])
#             with tab1:
#                 st.dataframe(
#                     df,
#                     height=600,
#                     use_container_width=True,
#                     hide_index=True,
#                     column_config={
#                         "ä¸­æ–‡": st.column_config.TextColumn(width="large"),
#                         "English": st.column_config.TextColumn(width="large")
#                     }
#                 )

#             with tab2:
#                 # ç”Ÿæˆæ–‡ä»¶æ•°æ®
#                 buffer = BytesIO()
#                 if export_format == "Excel":
#                     df.to_excel(buffer, index=False)
#                     mime = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#                     ext = "xlsx"
#                 elif export_format == "CSV":
#                     df.to_csv(buffer, index=False, encoding="utf-8-sig")
#                     mime = "text/csv"
#                     ext = "csv"
#                 elif export_format == "TXT":
#                     txt_content = "\n".join([f"{zh}\t{en}" for zh, en in aligned])
#                     buffer.write(txt_content.encode("utf-8"))
#                     mime = "text/plain"
#                     ext = "txt"
#                 elif export_format == "HTML":
#                     html_content = df.to_html(index=False)
#                     buffer.write(html_content.encode("utf-8"))
#                     mime = "text/html"
#                     ext = "html"
#                 elif export_format == "Markdown":
#                     md_content = df.to_markdown(index=False)
#                     buffer.write(md_content.encode("utf-8"))
#                     mime = "text/markdown"
#                     ext = "md"

#                 # ä¸‹è½½æŒ‰é’®
#                 st.download_button(
#                     label=f"ğŸ’¾ ä¸‹è½½ {export_format} æ–‡ä»¶",
#                     data=buffer.getvalue(),
#                     file_name=f"aligned_text.{ext}",
#                     mime=mime,
#                     use_container_width=True
#                 )

#             # ç»Ÿè®¡é¢æ¿
#             with st.expander("ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡", expanded=True):
#                 col1, col2, col3 = st.columns(3)
#                 col1.metric("å¯¹é½æ•°é‡", len(aligned))
#                 col2.metric("å¤„ç†è®¾å¤‡", "GPU âœ…" if use_gpu and torch.cuda.is_available() else "CPU")
#                 col3.metric("å†…å­˜å ç”¨", f"{torch.cuda.memory_allocated()/1e6:.1f} MB" if use_gpu else "N/A")

# if __name__ == '__main__':
#     from streamlit.web import cli as stcli
#     from streamlit import runtime
#     import sys

#     st.set_page_config(
#         page_title="æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ",
#         page_icon="ğŸŒ",
#         layout="wide",
#         initial_sidebar_state="expanded"
#     )

#     if runtime.exists():
#         main()
#     else:
#         sys.argv = ["streamlit", "run", sys.argv[0]]
#         sys.exit(stcli.main())



import streamlit as st
import pandas as pd
import numpy as np
import re
import os
from io import BytesIO
from sklearn.metrics.pairwise import cosine_similarity
from fastdtw import fastdtw
from sentence_transformers import SentenceTransformer
import torch
from datetime import datetime

# é…ç½®è®¾ç½®
os.environ["TRANSFORMERS_OFFLINE"] = "1"
# st.set_page_config(
#     page_title="æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ",
#     page_icon="ğŸŒ",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )

def main():
    # ========== ç•Œé¢ç»„ä»¶ ==========
    st.title("ğŸŒ æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ")
    st.caption("ä¸“ä¸šçº§ä¸­è‹±æ–‡æ–‡æ¡£å¯¹é½å·¥å…· | æ”¯æŒGPUåŠ é€Ÿ")

    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        use_gpu = st.checkbox("å¯ç”¨GPUåŠ é€Ÿ", value=torch.cuda.is_available())
        export_format = st.selectbox(
            "å¯¼å‡ºæ ¼å¼",
            ["Excel", "CSV", "TXT", "HTML", "Markdown"],
            index=0
        )
        
        st.markdown("---")
        st.header("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
        zh_file = st.file_uploader("ä¸Šä¼ ä¸­æ–‡æ–‡æ¡£", type=["txt"], key="zh")
        en_file = st.file_uploader("ä¸Šä¼ è‹±æ–‡æ–‡æ¡£", type=["txt"], key="en")

    # ========== æ ¸å¿ƒåŠŸèƒ½ ==========
    def split_sentences(text, lang):
        """æ™ºèƒ½åˆ†å¥å‡½æ•°"""
        start_time = datetime.now()
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ?ï¼])([^â€â€™])', r'\1\n\2', text) if lang == "zh" \
            else re.sub(r'([.!?])([â€™"])', r'\1\n\2', text)
        sentences = [s.strip() for s in re.split(r'\n+', text) if s.strip()]
        st.write(f"ğŸ”  åˆ†å¥å®Œæˆ | è€—æ—¶ {(datetime.now()-start_time).total_seconds():.2f}s")
        return sentences

    def align_texts(zh_sentences, en_sentences):
        """GPUåŠ é€Ÿçš„å¯¹é½å‡½æ•°"""
        # è®¾å¤‡æ£€æµ‹ä¸é€‰æ‹©
        device = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"
        st.write(f"âš™ï¸ ä½¿ç”¨è®¾å¤‡: {device.upper()}")

        # æ¨¡å‹åŠ è½½
        model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
        st.write("ğŸ” æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡å‹...")
        try:
            model_load_start = datetime.now()
            model = SentenceTransformer(model_path, device=device)
            st.write(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ | è€—æ—¶ {(datetime.now()-model_load_start).total_seconds():.2f}s")
        except Exception as e:
            raise RuntimeError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

        # æ–‡æœ¬ç¼–ç 
        st.write("ğŸ“¡ æ­£åœ¨ç¼–ç æ–‡æœ¬...")
        encode_start = datetime.now()
        zh_embeddings = model.encode(zh_sentences, show_progress_bar=False)
        en_embeddings = model.encode(en_sentences, show_progress_bar=False)
        st.write(f"âœ… ç¼–ç å®Œæˆ | è€—æ—¶ {(datetime.now()-encode_start).total_seconds():.2f}s")
        st.write(f"ğŸ“Š ä¸­æ–‡åµŒå…¥ç»´åº¦: {zh_embeddings.shape} | è‹±æ–‡åµŒå…¥ç»´åº¦: {en_embeddings.shape}")

        # å¯¹é½è®¡ç®—
        st.write("â³ æ­£åœ¨è®¡ç®—å¯¹é½è·¯å¾„...")
        def cosine_distance(a, b):
            return 1 - cosine_similarity([a], [b])[0][0]

        dtw_start = datetime.now()
        try:
            _, path = fastdtw(zh_embeddings, en_embeddings, 
                             dist=cosine_distance, radius=3)
            st.write(f"âœ… å¯¹é½è®¡ç®—å®Œæˆ | è€—æ—¶ {(datetime.now()-dtw_start).total_seconds():.2f}s")
        except Exception as e:
            raise RuntimeError(f"âŒ å¯¹é½è®¡ç®—å¤±è´¥: {str(e)}")

        # æ„å»ºå¯¹é½ç»“æœ
        st.write("ğŸ”— æ­£åœ¨æ„å»ºå¯¹é½ç»“æœ...")
        build_start = datetime.now()
        aligned_pairs = []
        last_zh, last_en = -1, -1
        
        for i, j in path:
            if i != last_zh and j != last_en:
                aligned_pairs.append((zh_sentences[i], en_sentences[j]))
                last_zh, last_en = i, j
            elif i == last_zh:
                aligned_pairs[-1] = (aligned_pairs[-1][0], 
                                    aligned_pairs[-1][1] + " " + en_sentences[j])
                last_en = j
            elif j == last_en:
                aligned_pairs[-1] = (aligned_pairs[-1][0] + zh_sentences[i], 
                                    aligned_pairs[-1][1])
                last_zh = i
        st.write(f"âœ… ç»“æœæ„å»ºå®Œæˆ | è€—æ—¶ {(datetime.now()-build_start).total_seconds():.2f}s")
        return aligned_pairs

    # ========== æ–‡ä»¶å¤„ç† ==========
    @st.cache_data(show_spinner=False)
    def load_file(uploaded_file, default_path):
        """å¸¦ç¼“å­˜çš„æ–‡ä»¶åŠ è½½"""
        if uploaded_file is not None:
            return uploaded_file.read().decode("utf-8")
        try:
            with open(default_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            return None

    # ========== ä¸»æµç¨‹ ==========
    zh_text = load_file(zh_file, "zn.txt")
    en_text = load_file(en_file, "en.txt")

    if not zh_text or not en_text:
        missing = []
        if not zh_text: missing.append("ä¸­æ–‡æ–‡æ¡£")
        if not en_text: missing.append("è‹±æ–‡æ–‡æ¡£")
        st.error(f"âŒ ç¼ºå°‘ {' å’Œ '.join(missing)}")
        return

    if st.button("ğŸš€ å¼€å§‹å¤„ç†", use_container_width=True, type="primary"):
        process_start = datetime.now()
        with st.status("ğŸ“Š å¤„ç†æµç¨‹", expanded=True) as status:
            try:
                # åˆ†å¥å¤„ç†
                st.write("## é˜¶æ®µ 1/4ï¼šåˆ†å¥å¤„ç†")
                zh_sents = split_sentences(zh_text, "zh")
                en_sents = split_sentences(en_text, "en")
                st.write(f"- ä¸­æ–‡å¥å­æ•°ï¼š{len(zh_sents)}")
                st.write(f"- è‹±æ–‡å¥å­æ•°ï¼š{len(en_sents)}")

                # æ‰§è¡Œå¯¹é½
                st.write("## é˜¶æ®µ 2/4ï¼šè¯­ä¹‰å¯¹é½")
                aligned = align_texts(zh_sents, en_sents)
                
                # ç»“æœå¤„ç†
                st.write("## é˜¶æ®µ 3/4ï¼šç»“æœå¤„ç†")
                df = pd.DataFrame(aligned, columns=["ä¸­æ–‡", "English"])
                
                # æ–‡ä»¶å¯¼å‡º
                st.write("## é˜¶æ®µ 4/4ï¼šæ–‡ä»¶å¯¼å‡º")
                buffer = BytesIO()
                if export_format == "Excel":
                    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                        df.to_excel(writer, index=False)
                    mime_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    ext = "xlsx"
                elif export_format == "CSV":
                    buffer.write(df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8'))
                    mime_type = "text/csv"
                    ext = "csv"
                elif export_format == "TXT":
                    txt_content = "\n".join([f"{zh}\t{en}" for zh, en in aligned])
                    buffer.write(txt_content.encode("utf-8"))
                    mime_type = "text/plain"
                    ext = "txt"
                elif export_format == "HTML":
                    html_content = df.to_html(index=False, border=0)
                    buffer.write(html_content.encode("utf-8"))
                    mime_type = "text/html"
                    ext = "html"
                elif export_format == "Markdown":
                    md_content = df.to_markdown(index=False)
                    buffer.write(md_content.encode("utf-8"))
                    mime_type = "text/markdown"
                    ext = "md"

                total_time = (datetime.now() - process_start).total_seconds()
                status.update(
                    label=f"âœ… å¤„ç†å®Œæˆ | æ€»è€—æ—¶ {total_time:.2f}s",
                    state="complete",
                    expanded=False
                )

            except Exception as e:
                status.update(label="âŒ å¤„ç†å¤±è´¥", state="error")
                st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
                st.stop()

        # ç»“æœæ˜¾ç¤º
        st.success(f"æˆåŠŸå¯¹é½ {len(aligned)} å¯¹å¥å­")
        with st.container():
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.subheader("ğŸ“„ æ•°æ®é¢„è§ˆ")
                st.dataframe(
                    df,
                    height=600,
                    use_container_width=True,
                    hide_index=True
                )
            
            with col2:
                st.subheader("ğŸ“¥ å¯¼å‡ºé€‰é¡¹")
                st.download_button(
                    label=f"ä¸‹è½½ {export_format} æ–‡ä»¶",
                    data=buffer.getvalue(),
                    file_name=f"aligned_text.{ext}",
                    mime=mime_type,
                    use_container_width=True
                )
                
                # æ€§èƒ½ç»Ÿè®¡
                st.subheader("ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡")
                st.metric("æ€»å¤„ç†æ—¶é—´", f"{total_time:.2f}ç§’")
                st.metric("å¤„ç†è®¾å¤‡", "GPU ğŸš€" if torch.cuda.is_available() else "CPU ğŸ’»")
                if torch.cuda.is_available():
                    st.metric("æ˜¾å­˜å ç”¨", f"{torch.cuda.memory_allocated()/1e6:.1f} MB")

if __name__ == '__main__':
    from streamlit.web import cli as stcli
    from streamlit import runtime
    import sys

    st.set_page_config(
        page_title="æ™ºèƒ½åŒè¯­å¯¹é½ç³»ç»Ÿ",
        page_icon="ğŸŒ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    if runtime.exists():
        main()
    else:
        sys.argv = ["streamlit", "run", sys.argv[0]]
        sys.exit(stcli.main())